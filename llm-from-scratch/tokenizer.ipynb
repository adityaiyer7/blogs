{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582f8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ee284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog. This sentence contains every letter of the alphabet at least once, making it perfect for testing various algorithms and systems. \n",
      "\n",
      "Machine learning has revolutionized the way we approach complex problems in computer science. From natural language processing to computer vision, these algorithms can identify patterns in data that would be impossible for humans to detect manually.\n",
      "\n",
      "The transformer architecture, introduced in the paper \"Attention is All You Need,\" has become the foundation for modern language models. These models use self-attention mechanisms to process sequences of tokens, allowing them to understand context and relationships between words in a sentence.\n",
      "\n",
      "Tokenization is a crucial preprocessing step in natural language processing. It involves breaking down text into smaller units called tokens, which can be words, subwords, or even individual characters. Different tokenization strategies have different advantages: word-level tokenization preserves semantic meaning but struggles with out-of-vocabulary words, while subword tokenization like BPE (Byte Pair Encoding) can handle rare words by breaking them into smaller pieces.\n",
      "\n",
      "The development of large language models like GPT, BERT, and T5 has shown remarkable capabilities in understanding and generating human-like text. These models are trained on vast amounts of text data and can perform tasks ranging from translation to question answering to creative writing.\n",
      "\n",
      "However, with great power comes great responsibility. The ethical implications of AI systems are becoming increasingly important as these technologies become more prevalent in society. Issues such as bias, fairness, transparency, and accountability must be carefully considered when deploying AI systems in real-world applications.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"The quick brown fox jumps over the lazy dog. This sentence contains every letter of the alphabet at least once, making it perfect for testing various algorithms and systems. \n",
    "\n",
    "Machine learning has revolutionized the way we approach complex problems in computer science. From natural language processing to computer vision, these algorithms can identify patterns in data that would be impossible for humans to detect manually.\n",
    "\n",
    "The transformer architecture, introduced in the paper \"Attention is All You Need,\" has become the foundation for modern language models. These models use self-attention mechanisms to process sequences of tokens, allowing them to understand context and relationships between words in a sentence.\n",
    "\n",
    "Tokenization is a crucial preprocessing step in natural language processing. It involves breaking down text into smaller units called tokens, which can be words, subwords, or even individual characters. Different tokenization strategies have different advantages: word-level tokenization preserves semantic meaning but struggles with out-of-vocabulary words, while subword tokenization like BPE (Byte Pair Encoding) can handle rare words by breaking them into smaller pieces.\n",
    "\n",
    "The development of large language models like GPT, BERT, and T5 has shown remarkable capabilities in understanding and generating human-like text. These models are trained on vast amounts of text data and can perform tasks ranging from translation to question answering to creative writing.\n",
    "\n",
    "However, with great power comes great responsibility. The ethical implications of AI systems are becoming increasingly important as these technologies become more prevalent in society. Issues such as bias, fairness, transparency, and accountability must be carefully considered when deploying AI systems in real-world applications.\"\"\"\n",
    "\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b5963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 195\n",
      "Total tokens: 304\n"
     ]
    }
   ],
   "source": [
    "# Split text into words and punctuation (excluding whitespace)\n",
    "tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', sample_text)  # Extract words and punctuation, exclude whitespace\n",
    "unique_tokens = list(set(tokens))  # Get unique tokens\n",
    "print(f\"Number of unique tokens: {len(unique_tokens)}\")\n",
    "print(f\"Total tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c11c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'called', ')', 'T5', 'from', 'great', 'impossible', 'revolutionized', 'computer', 'with', 'on', 'perform', 'such', 'pieces', 'Encoding', 'allowing', 'of', 'models', 'introduced', 'language', 'transparency', 'least', 'considered', 'breaking', 'be', 'modern', 'has', 'contains', 'translation', 'writing', 'systems', 'various', 'Pair', 'society', 'words', 'transformer', 'increasingly', 'prevalent', 'would', 'These', 'is', 'ranging', 'jumps', 'answering', 'brown', 'handle', 'Attention', '.', 'fox', 'applications', 'The', 'sequences', 'crucial', 'meaning', 'fairness', 'or', 'generating', 'trained', 'detect', 'Issues', 'BPE', 'into', 'the', 'shown', 'capabilities', 'patterns', 'more', 'between', 'a', 'semantic', 'architecture', 'preprocessing', 'Different', 'strategies', 'word', 'like', 'these', 'to', 'for', 'understand', 'struggles', 'manually', 'becoming', 'lazy', 'and', 'human', 'important', 'as', 'but', 'humans', 'testing', 'characters', 'remarkable', 'subwords', 'processing', 'level', 'real', 'self', 'AI', 'All', 'From', 'attention', 'problems', 'them', 'It', 'accountability', 'must', 'can', 'which', 'tokenization', 'that', ':', 'tasks', ',', 'making', '(', 'out', 'Machine', 'step', 'development', 'individual', 'by', 'over', 'Byte', 'understanding', 'comes', '\"', 'every', 'Need', 'perfect', 'implications', 'bias', 'letter', 'paper', 'once', 'relationships', 'we', 'power', 'technologies', 'vocabulary', 'algorithms', 'text', 'approach', 'This', 'amounts', 'identify', 'preserves', 'context', 'dog', 'even', 'units', 'You', 'However', '-', 'ethical', 'complex', 'way', 'while', 'deploying', 'Tokenization', 'learning', 'alphabet', 'advantages', 'sentence', 'carefully', 'smaller', 'science', 'down', 'creative', 'subword', 'data', 'world', 'at', 'vision', 'mechanisms', 'become', 'quick', 'different', 'GPT', 'have', 'are', 'process', 'natural', 'tokens', 'BERT', 'responsibility', 'involves', 'vast', 'in', 'foundation', 'use', 'it', 'large', 'when', 'rare']\n"
     ]
    }
   ],
   "source": [
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a95957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 0,\n",
       " 'called': 1,\n",
       " ')': 2,\n",
       " 'T5': 3,\n",
       " 'from': 4,\n",
       " 'great': 5,\n",
       " 'impossible': 6,\n",
       " 'revolutionized': 7,\n",
       " 'computer': 8,\n",
       " 'with': 9,\n",
       " 'on': 10,\n",
       " 'perform': 11,\n",
       " 'such': 12,\n",
       " 'pieces': 13,\n",
       " 'Encoding': 14,\n",
       " 'allowing': 15,\n",
       " 'of': 16,\n",
       " 'models': 17,\n",
       " 'introduced': 18,\n",
       " 'language': 19,\n",
       " 'transparency': 20,\n",
       " 'least': 21,\n",
       " 'considered': 22,\n",
       " 'breaking': 23,\n",
       " 'be': 24,\n",
       " 'modern': 25,\n",
       " 'has': 26,\n",
       " 'contains': 27,\n",
       " 'translation': 28,\n",
       " 'writing': 29,\n",
       " 'systems': 30,\n",
       " 'various': 31,\n",
       " 'Pair': 32,\n",
       " 'society': 33,\n",
       " 'words': 34,\n",
       " 'transformer': 35,\n",
       " 'increasingly': 36,\n",
       " 'prevalent': 37,\n",
       " 'would': 38,\n",
       " 'These': 39,\n",
       " 'is': 40,\n",
       " 'ranging': 41,\n",
       " 'jumps': 42,\n",
       " 'answering': 43,\n",
       " 'brown': 44,\n",
       " 'handle': 45,\n",
       " 'Attention': 46,\n",
       " '.': 47,\n",
       " 'fox': 48,\n",
       " 'applications': 49,\n",
       " 'The': 50,\n",
       " 'sequences': 51,\n",
       " 'crucial': 52,\n",
       " 'meaning': 53,\n",
       " 'fairness': 54,\n",
       " 'or': 55,\n",
       " 'generating': 56,\n",
       " 'trained': 57,\n",
       " 'detect': 58,\n",
       " 'Issues': 59,\n",
       " 'BPE': 60,\n",
       " 'into': 61,\n",
       " 'the': 62,\n",
       " 'shown': 63,\n",
       " 'capabilities': 64,\n",
       " 'patterns': 65,\n",
       " 'more': 66,\n",
       " 'between': 67,\n",
       " 'a': 68,\n",
       " 'semantic': 69,\n",
       " 'architecture': 70,\n",
       " 'preprocessing': 71,\n",
       " 'Different': 72,\n",
       " 'strategies': 73,\n",
       " 'word': 74,\n",
       " 'like': 75,\n",
       " 'these': 76,\n",
       " 'to': 77,\n",
       " 'for': 78,\n",
       " 'understand': 79,\n",
       " 'struggles': 80,\n",
       " 'manually': 81,\n",
       " 'becoming': 82,\n",
       " 'lazy': 83,\n",
       " 'and': 84,\n",
       " 'human': 85,\n",
       " 'important': 86,\n",
       " 'as': 87,\n",
       " 'but': 88,\n",
       " 'humans': 89,\n",
       " 'testing': 90,\n",
       " 'characters': 91,\n",
       " 'remarkable': 92,\n",
       " 'subwords': 93,\n",
       " 'processing': 94,\n",
       " 'level': 95,\n",
       " 'real': 96,\n",
       " 'self': 97,\n",
       " 'AI': 98,\n",
       " 'All': 99,\n",
       " 'From': 100,\n",
       " 'attention': 101,\n",
       " 'problems': 102,\n",
       " 'them': 103,\n",
       " 'It': 104,\n",
       " 'accountability': 105,\n",
       " 'must': 106,\n",
       " 'can': 107,\n",
       " 'which': 108,\n",
       " 'tokenization': 109,\n",
       " 'that': 110,\n",
       " ':': 111,\n",
       " 'tasks': 112,\n",
       " ',': 113,\n",
       " 'making': 114,\n",
       " '(': 115,\n",
       " 'out': 116,\n",
       " 'Machine': 117,\n",
       " 'step': 118,\n",
       " 'development': 119,\n",
       " 'individual': 120,\n",
       " 'by': 121,\n",
       " 'over': 122,\n",
       " 'Byte': 123,\n",
       " 'understanding': 124,\n",
       " 'comes': 125,\n",
       " '\"': 126,\n",
       " 'every': 127,\n",
       " 'Need': 128,\n",
       " 'perfect': 129,\n",
       " 'implications': 130,\n",
       " 'bias': 131,\n",
       " 'letter': 132,\n",
       " 'paper': 133,\n",
       " 'once': 134,\n",
       " 'relationships': 135,\n",
       " 'we': 136,\n",
       " 'power': 137,\n",
       " 'technologies': 138,\n",
       " 'vocabulary': 139,\n",
       " 'algorithms': 140,\n",
       " 'text': 141,\n",
       " 'approach': 142,\n",
       " 'This': 143,\n",
       " 'amounts': 144,\n",
       " 'identify': 145,\n",
       " 'preserves': 146,\n",
       " 'context': 147,\n",
       " 'dog': 148,\n",
       " 'even': 149,\n",
       " 'units': 150,\n",
       " 'You': 151,\n",
       " 'However': 152,\n",
       " '-': 153,\n",
       " 'ethical': 154,\n",
       " 'complex': 155,\n",
       " 'way': 156,\n",
       " 'while': 157,\n",
       " 'deploying': 158,\n",
       " 'Tokenization': 159,\n",
       " 'learning': 160,\n",
       " 'alphabet': 161,\n",
       " 'advantages': 162,\n",
       " 'sentence': 163,\n",
       " 'carefully': 164,\n",
       " 'smaller': 165,\n",
       " 'science': 166,\n",
       " 'down': 167,\n",
       " 'creative': 168,\n",
       " 'subword': 169,\n",
       " 'data': 170,\n",
       " 'world': 171,\n",
       " 'at': 172,\n",
       " 'vision': 173,\n",
       " 'mechanisms': 174,\n",
       " 'become': 175,\n",
       " 'quick': 176,\n",
       " 'different': 177,\n",
       " 'GPT': 178,\n",
       " 'have': 179,\n",
       " 'are': 180,\n",
       " 'process': 181,\n",
       " 'natural': 182,\n",
       " 'tokens': 183,\n",
       " 'BERT': 184,\n",
       " 'responsibility': 185,\n",
       " 'involves': 186,\n",
       " 'vast': 187,\n",
       " 'in': 188,\n",
       " 'foundation': 189,\n",
       " 'use': 190,\n",
       " 'it': 191,\n",
       " 'large': 192,\n",
       " 'when': 193,\n",
       " 'rare': 194}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = {word:idx for idx, word in enumerate(unique_tokens)}\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc7d7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(text):\n",
    "    return re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071ada5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input):\n",
    "    preprocessed_input = preprocess_input(input)\n",
    "    encoded_tokens = []\n",
    "    for token in preprocessed_input:\n",
    "        encoded_tokens.append(vocabulary[token])\n",
    "    return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ba2117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[152, 124, 160, 55]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(\"However understanding learning or\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a82470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'question',\n",
       " 1: 'called',\n",
       " 2: ')',\n",
       " 3: 'T5',\n",
       " 4: 'from',\n",
       " 5: 'great',\n",
       " 6: 'impossible',\n",
       " 7: 'revolutionized',\n",
       " 8: 'computer',\n",
       " 9: 'with',\n",
       " 10: 'on',\n",
       " 11: 'perform',\n",
       " 12: 'such',\n",
       " 13: 'pieces',\n",
       " 14: 'Encoding',\n",
       " 15: 'allowing',\n",
       " 16: 'of',\n",
       " 17: 'models',\n",
       " 18: 'introduced',\n",
       " 19: 'language',\n",
       " 20: 'transparency',\n",
       " 21: 'least',\n",
       " 22: 'considered',\n",
       " 23: 'breaking',\n",
       " 24: 'be',\n",
       " 25: 'modern',\n",
       " 26: 'has',\n",
       " 27: 'contains',\n",
       " 28: 'translation',\n",
       " 29: 'writing',\n",
       " 30: 'systems',\n",
       " 31: 'various',\n",
       " 32: 'Pair',\n",
       " 33: 'society',\n",
       " 34: 'words',\n",
       " 35: 'transformer',\n",
       " 36: 'increasingly',\n",
       " 37: 'prevalent',\n",
       " 38: 'would',\n",
       " 39: 'These',\n",
       " 40: 'is',\n",
       " 41: 'ranging',\n",
       " 42: 'jumps',\n",
       " 43: 'answering',\n",
       " 44: 'brown',\n",
       " 45: 'handle',\n",
       " 46: 'Attention',\n",
       " 47: '.',\n",
       " 48: 'fox',\n",
       " 49: 'applications',\n",
       " 50: 'The',\n",
       " 51: 'sequences',\n",
       " 52: 'crucial',\n",
       " 53: 'meaning',\n",
       " 54: 'fairness',\n",
       " 55: 'or',\n",
       " 56: 'generating',\n",
       " 57: 'trained',\n",
       " 58: 'detect',\n",
       " 59: 'Issues',\n",
       " 60: 'BPE',\n",
       " 61: 'into',\n",
       " 62: 'the',\n",
       " 63: 'shown',\n",
       " 64: 'capabilities',\n",
       " 65: 'patterns',\n",
       " 66: 'more',\n",
       " 67: 'between',\n",
       " 68: 'a',\n",
       " 69: 'semantic',\n",
       " 70: 'architecture',\n",
       " 71: 'preprocessing',\n",
       " 72: 'Different',\n",
       " 73: 'strategies',\n",
       " 74: 'word',\n",
       " 75: 'like',\n",
       " 76: 'these',\n",
       " 77: 'to',\n",
       " 78: 'for',\n",
       " 79: 'understand',\n",
       " 80: 'struggles',\n",
       " 81: 'manually',\n",
       " 82: 'becoming',\n",
       " 83: 'lazy',\n",
       " 84: 'and',\n",
       " 85: 'human',\n",
       " 86: 'important',\n",
       " 87: 'as',\n",
       " 88: 'but',\n",
       " 89: 'humans',\n",
       " 90: 'testing',\n",
       " 91: 'characters',\n",
       " 92: 'remarkable',\n",
       " 93: 'subwords',\n",
       " 94: 'processing',\n",
       " 95: 'level',\n",
       " 96: 'real',\n",
       " 97: 'self',\n",
       " 98: 'AI',\n",
       " 99: 'All',\n",
       " 100: 'From',\n",
       " 101: 'attention',\n",
       " 102: 'problems',\n",
       " 103: 'them',\n",
       " 104: 'It',\n",
       " 105: 'accountability',\n",
       " 106: 'must',\n",
       " 107: 'can',\n",
       " 108: 'which',\n",
       " 109: 'tokenization',\n",
       " 110: 'that',\n",
       " 111: ':',\n",
       " 112: 'tasks',\n",
       " 113: ',',\n",
       " 114: 'making',\n",
       " 115: '(',\n",
       " 116: 'out',\n",
       " 117: 'Machine',\n",
       " 118: 'step',\n",
       " 119: 'development',\n",
       " 120: 'individual',\n",
       " 121: 'by',\n",
       " 122: 'over',\n",
       " 123: 'Byte',\n",
       " 124: 'understanding',\n",
       " 125: 'comes',\n",
       " 126: '\"',\n",
       " 127: 'every',\n",
       " 128: 'Need',\n",
       " 129: 'perfect',\n",
       " 130: 'implications',\n",
       " 131: 'bias',\n",
       " 132: 'letter',\n",
       " 133: 'paper',\n",
       " 134: 'once',\n",
       " 135: 'relationships',\n",
       " 136: 'we',\n",
       " 137: 'power',\n",
       " 138: 'technologies',\n",
       " 139: 'vocabulary',\n",
       " 140: 'algorithms',\n",
       " 141: 'text',\n",
       " 142: 'approach',\n",
       " 143: 'This',\n",
       " 144: 'amounts',\n",
       " 145: 'identify',\n",
       " 146: 'preserves',\n",
       " 147: 'context',\n",
       " 148: 'dog',\n",
       " 149: 'even',\n",
       " 150: 'units',\n",
       " 151: 'You',\n",
       " 152: 'However',\n",
       " 153: '-',\n",
       " 154: 'ethical',\n",
       " 155: 'complex',\n",
       " 156: 'way',\n",
       " 157: 'while',\n",
       " 158: 'deploying',\n",
       " 159: 'Tokenization',\n",
       " 160: 'learning',\n",
       " 161: 'alphabet',\n",
       " 162: 'advantages',\n",
       " 163: 'sentence',\n",
       " 164: 'carefully',\n",
       " 165: 'smaller',\n",
       " 166: 'science',\n",
       " 167: 'down',\n",
       " 168: 'creative',\n",
       " 169: 'subword',\n",
       " 170: 'data',\n",
       " 171: 'world',\n",
       " 172: 'at',\n",
       " 173: 'vision',\n",
       " 174: 'mechanisms',\n",
       " 175: 'become',\n",
       " 176: 'quick',\n",
       " 177: 'different',\n",
       " 178: 'GPT',\n",
       " 179: 'have',\n",
       " 180: 'are',\n",
       " 181: 'process',\n",
       " 182: 'natural',\n",
       " 183: 'tokens',\n",
       " 184: 'BERT',\n",
       " 185: 'responsibility',\n",
       " 186: 'involves',\n",
       " 187: 'vast',\n",
       " 188: 'in',\n",
       " 189: 'foundation',\n",
       " 190: 'use',\n",
       " 191: 'it',\n",
       " 192: 'large',\n",
       " 193: 'when',\n",
       " 194: 'rare'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create reverse mapping from vocabulary\n",
    "int_to_string_mapper = {idx: word for word, idx in vocabulary.items()}\n",
    "int_to_string_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2056cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(ids):\n",
    "    \"\"\"Decode token indices back to text\"\"\"\n",
    "    # Convert indices to tokens\n",
    "    tokens = [int_to_string_mapper[id] for id in ids if id in int_to_string_mapper]\n",
    "    \n",
    "    # Post-processing: reconstruct text with proper spacing\n",
    "    result = \"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Add space before words (but not before punctuation)\n",
    "        if i > 0 and token.isalnum() and tokens[i-1].isalnum():\n",
    "            result += \" \"\n",
    "        result += token\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5b262e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question impossible revolutionized involves'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder([0, 6, 7, 186])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d66150",
   "metadata": {},
   "source": [
    "Obviously, the drawback here is that we are not able to handle out-of-vocabulary words. Let's package this neatly into a class and also add support for out-of-vocabulary words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d67f720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocabulary):\n",
    "        \"\"\"Initialize the tokenizer with a given vocabulary mapping.\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocabulary : dict\n",
    "            A dictionary mapping tokens (str) to unique integer IDs.\n",
    "        Adds an <_unkown_> token to handle out-of-vocabulary terms.\n",
    "        \"\"\"\n",
    "        self.word_to_id = vocabulary\n",
    "        self.id_to_word = {idx:word for word,idx in vocabulary.items()}\n",
    "        # let's add a <_unkown_> token to our mappers\n",
    "        self.word_to_id[\"<_unkown_>\"] = len(vocabulary) + 1\n",
    "        self.id_to_word[len(vocabulary) + 1] = \"<_unkown_>\"\n",
    "    \n",
    "    def preprocess_input(self, text):\n",
    "        \"\"\"Tokenize input text into a list of words and punctuation.\n",
    "        Whitespace is ignored. This uses the same regex pattern as the\n",
    "        vocabulary builder to ensure consistency.\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            Raw input string to be tokenized.\n",
    "        Returns\n",
    "        -------\n",
    "        list[str]\n",
    "            List of tokens (words and punctuation).\n",
    "        \"\"\"\n",
    "        return re.findall(r'\\b\\w+\\b|[^\\w\\s]', text) \n",
    "    \n",
    "    def postprocess_text(self, tokens):\n",
    "        \"\"\"Reconstruct text from a list of tokens.\n",
    "        Inserts spaces between consecutive alphanumeric tokens while keeping\n",
    "        punctuation attached to the preceding token.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : list[str]\n",
    "            List of tokens produced by the tokenizer.\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Human-readable string rebuilt from the tokens.\n",
    "        \"\"\"\n",
    "        result = \"\"\n",
    "        for i, token in enumerate(tokens):\n",
    "        # Add space before words (but not before punctuation)\n",
    "            if i > 0 and token.isalnum() and tokens[i-1].isalnum():\n",
    "                result += \" \"\n",
    "            result += token\n",
    "        return result\n",
    "                \n",
    "    def encoder(self, input):\n",
    "        \"\"\"Convert raw text to a list of token IDs.\n",
    "        Any token not present in the vocabulary is mapped to the\n",
    "        <_unknown_> token ID.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : str\n",
    "            Raw input string.\n",
    "        Returns\n",
    "        -------\n",
    "        list[int]\n",
    "            Sequence of token IDs corresponding to the input text.\n",
    "        \"\"\"\n",
    "        preprocessed_input = self.preprocess_input(input)\n",
    "        encoded_tokens = []\n",
    "        for token in preprocessed_input:\n",
    "            if token not in self.word_to_id:\n",
    "                encoded_tokens.append(self.word_to_id[\"<_unkown_>\"])\n",
    "            else:\n",
    "                encoded_tokens.append(self.word_to_id[token])\n",
    "        return encoded_tokens\n",
    "    \n",
    "    def decoder(self, ids):\n",
    "        \"\"\"Convert a sequence of token IDs back to human-readable text.\n",
    "        Parameters\n",
    "        ----------\n",
    "        ids : list[int]\n",
    "            Sequence of token IDs.\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Decoded text.\n",
    "        \"\"\"\n",
    "        tokens = [self.id_to_word[id] for id in ids if id in self.id_to_word]\n",
    "        # Post-processing: reconstruct text with proper spacing\n",
    "        return self.postprocess_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1685040b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: However understanding learning or\n",
      "Encoded IDs: [152, 124, 160, 55]\n",
      "Decoded text: However understanding learning or\n",
      "\n",
      "Out-of-vocabulary test: Machine learning with Python programming\n",
      "Encoded IDs: [117, 160, 9, 197, 197]\n",
      "Decoded text: Machine learning with<_unkown_><_unkown_>\n",
      "\n",
      "Full round-trip test:\n",
      "Original: The quick brown fox jumps over the lazy dog.\n",
      "Encoded: [50, 176, 44, 48, 42, 122, 62, 83, 148, 47]\n",
      "Decoded: The quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocabulary)\n",
    "\n",
    "test_text = \"However understanding learning or\"\n",
    "encoded = tokenizer.encoder(test_text)\n",
    "print(f\"Original text: {test_text}\")\n",
    "print(f\"Encoded IDs: {encoded}\")\n",
    "\n",
    "decoded = tokenizer.decoder(encoded)\n",
    "print(f\"Decoded text: {decoded}\")\n",
    "\n",
    "oov_text = \"Machine learning with Python programming\"\n",
    "encoded_oov = tokenizer.encoder(oov_text)\n",
    "print(f\"\\nOut-of-vocabulary test: {oov_text}\")\n",
    "print(f\"Encoded IDs: {encoded_oov}\")\n",
    "decoded_oov = tokenizer.decoder(encoded_oov)\n",
    "print(f\"Decoded text: {decoded_oov}\")\n",
    "\n",
    "full_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "print(f\"\\nFull round-trip test:\")\n",
    "print(f\"Original: {full_text}\")\n",
    "encoded_full = tokenizer.encoder(full_text)\n",
    "print(f\"Encoded: {encoded_full}\")\n",
    "decoded_full = tokenizer.decoder(encoded_full)\n",
    "print(f\"Decoded: {decoded_full}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe37535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
